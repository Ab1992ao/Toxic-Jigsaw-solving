{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pretrained BERT models\nhttps://www.kaggle.com/sagol79/bert-inference-2/downloads/bert-inference-2.zip/4"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom numba import cuda\n\nimport sys\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)\nimport torch.utils.data\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nimport gc\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\n\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\nMAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nBATCH_SIZE = 32\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nLARGE_BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pretrained BERT models - Google's pretrained BERT model\nBERT_SMALL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nBERT_LARGE_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\n\n# JIGSAW fine-tuned BERT models\nJIGSAW_BERT_SMALL_MODEL_PATH = '../input/bert-inference-2/bert_2/bert_pytorch.bin'\nJIGSAW_BERT_LARGE_MODEL_PATH = '../input/pretrained-b-j/jigsaw-bert-large-uncased-len-220-fp16/epoch-1/pytorch_model.bin'\nJIGSAW_BERT_SMALL_JSON_PATH = '../input/bert-inference-2/bert_2/bert_config.json'\nJIGSAW_BERT_LARGE_JSON_PATH = '../input/pretrained-b-j/jigsaw-bert-large-uncased-len-220-fp16/epoch-1/config.json'\nNUM_BERT_MODELS = 1 #2\nINFER_BATCH_SIZE = 64\n\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\ntest_preds = np.zeros((test_df.shape[0],NUM_BERT_MODELS))\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\n\nprint(\"Predicting BERT large model......\")\n\n# Prepare data\nbert_config = BertConfig(JIGSAW_BERT_LARGE_JSON_PATH)\ntokenizer = BertTokenizer.from_pretrained(BERT_LARGE_PATH, cache_dir=None,do_lower_case=True)\nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))      \n\n# Load fine-tuned BERT model\ngc.collect()\nmodel = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(JIGSAW_BERT_LARGE_MODEL_PATH))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\n# Predicting\nmodel_preds = np.zeros((len(X_test)))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=INFER_BATCH_SIZE, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        model_preds[i * INFER_BATCH_SIZE:(i + 1) * INFER_BATCH_SIZE] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_preds[:,0] = torch.sigmoid(torch.tensor(model_preds)).numpy().ravel()\ndel model\ngc.collect()\n\n\"\"\" print(\"Predicting BERT small model......\")\nbert_config = BertConfig(JIGSAW_BERT_SMALL_JSON_PATH)\ntokenizer = BertTokenizer.from_pretrained(BERT_SMALL_PATH, cache_dir=None,do_lower_case=True)\nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))      \n\n# # # Load fine-tuned BERT model\nmodel = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(JIGSAW_BERT_SMALL_MODEL_PATH))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\n# Predicting\nmodel_preds = np.zeros((len(X_test)))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=INFER_BATCH_SIZE, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        model_preds[i * INFER_BATCH_SIZE:(i + 1) * INFER_BATCH_SIZE] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_preds[:,1] = torch.sigmoid(torch.tensor(model_preds)).numpy().ravel()\n\ndel model\ngc.collect()\"\"\"\n\n# Sub-model prediction\nbert_submission = pd.DataFrame.from_dict({\n'id': test_df['id'],\n'prediction': test_preds.mean(axis=1)})\nbert_submission.to_csv('bert_submission.csv', index=False)\ncuda.select_device(0)\ncuda.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Credits**\nThis notebook was mainly inspired by the following awesome kernel scripts:\n\nhttps://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n\nhttps://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage\n\nhttps://www.kaggle.com/shubham505/apply-by-simple-bilstm\n\n\n\n# Preparations\n\n## Datasets\nYou will need to add the following Kaggle dataset for pickled pretrained embeddings\n\nhttps://www.kaggle.com/chriscc/pickled-word-embedding\n\nhttps://www.kaggle.com/firewalkor/pretrained-b-j\n\n## Import packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport re\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nimport pickle\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import add\nfrom keras.layers import Input,Lambda, CuDNNLSTM, Convolution1D, GlobalMaxPooling1D ,GlobalAveragePooling1D, CuDNNGRU\nfrom keras.layers.embeddings import Embedding\nfrom keras import optimizers\nfrom keras.layers import Dense, concatenate, Activation, Dropout, SpatialDropout1D, Dot, Reshape, Bidirectional, BatchNormalization, Flatten\nfrom keras.models import Model, Sequential\nfrom keras.layers.convolutional import Conv1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.initializers import glorot_normal, orthogonal\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras\nfrom keras.utils import np_utils\nimport gc\nimport keras.backend as K\nfrom keras.optimizers import Adadelta\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn.metrics import f1_score\nfrom keras.losses import binary_crossentropy\nfrom tensorflow import set_random_seed\nfrom keras import backend\nimport tensorflow as tf\nimport string","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Configurations"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_PATHS = ['../input/pickled-word-embedding/crawl-300d-2M.pkl',\n                 '../input/pickled-word-embedding/glove.840B.300d.pkl']\n\n\nNUM_MODELS = 2 # The number of classifiers we want to train \nBATCH_SIZE = 512 # can be tuned\nLSTM_UNITS = 128 # can be tuned\nDENSE_HIDDEN_UNITS = 4*LSTM_UNITS # can betuned\nEPOCHS = 4 # The number of epoches we want to train for each classifier\nMAX_LEN = 220 # can ben tuned\nMAX_WORDS=450000 #best 450k\n\nIDENTITY_COLUMNS = [\n    'transgender', 'female', 'homosexual_gay_or_lesbian', 'muslim', 'hindu',\n    'white', 'black', 'psychiatric_or_mental_illness', 'jewish'\n    ]  \n\nAUX_COLUMNS = ['target', 'severe_toxicity','obscene','identity_attack','insult','threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------------------------- Preprocessing-------------------------------------#\nSYMBOLS_TO_ISOLATE = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\nSYMBOLS_TO_REMOVE = '\\n🍕\\r🐵\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n\nSYMBOLS_TO_REMOVE=set(SYMBOLS_TO_REMOVE)\nSYMBOLS_TO_ISOLATE=set(SYMBOLS_TO_ISOLATE)\n\nISOLATE_DICT = {ord(c):f' {c} ' for c in SYMBOLS_TO_ISOLATE}\nREMOVE_DICT = {ord(c):f'' for c in SYMBOLS_TO_REMOVE}\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n\nCHARS_TO_REMOVE=set(CHARS_TO_REMOVE)\n\nCONTRACTION_MAPPING ={\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n                        \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\",\n                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n                       \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n                       \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n                       \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n                       \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n                       \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n                       \"why've\": \"why have\",\"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n                        \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                        \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                       \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\"yay!\": \" good \",\n                        \"yay\": \" good \",\n                        \"yaay\": \" good \",\n                        \"yaaay\": \" good \",\n                        \"yaaaay\": \" good \",\n                        \"yaaaaay\": \" good \",\n                        \":/\": \" bad \",\n                        \":&gt;\": \" sad \",\n                        \":')\": \" sad \",\n                        \":-(\": \" frown \",\n                        \":(\": \" frown \",\n                        \":s\": \" frown \",\n                        \":-s\": \" frown \",\n                        \"&lt;3\": \" heart \",\n                        \":d\": \" smile \",\n                        \":p\": \" smile \",\n                        \":dd\": \" smile \",\n                        \"8)\": \" smile \",\n                        \":-)\": \" smile \",\n                        \":)\": \" smile \",\n                        \";)\": \" smile \",\n                        \"(-:\": \" smile \",\n                        \"(:\": \" smile \",\n                        \":/\": \" worry \",\n                        \":&gt;\": \" angry \",\n                        \":')\": \" sad \",\n                        \":-(\": \" sad \",\n                        \":(\": \" sad \",\n                        \":s\": \" sad \",\n                        \":-s\": \" sad \",\n                        r\"\\br\\b\": \"are\",\n                        r\"\\bu\\b\": \"you\",\n                        r\"\\bhaha\\b\": \"ha\",\n                        r\"\\bhaha\\b\": \"oh\",\n                        r\"\\bhh\\b\": \"h\",\n                        r\"\\bdon't\\b\": \"do not\",\n                        r\"\\bdoesn't\\b\": \"does not\",\n                        r\"\\bdidn't\\b\": \"did not\",\n                        r\"\\bhasn't\\b\": \"has not\",\n                        r\"\\bhaven't\\b\": \"have not\",\n                        r\"\\bhadn't\\b\": \"had not\",\n                        r\"\\bwon't\\b\": \"will not\",\n                        r\"\\bwouldn't\\b\": \"would not\",\n                        r\"\\bcan't\\b\": \"can not\",\n                        r\"\\bcannot\\b\": \"can not\",\n                        r\"\\bi'm\\b\": \"i am\",\n                        \"m\": \"am\",\n                        \"r\": \"are\",\n                        \"u\": \"you\",\n                        \"haha\": \"ha\",\n                        \"hahaha\": \"ha\",\n                        \"don't\": \"do not\",\n                        \"doesn't\": \"does not\",\n                        \"didn't\": \"did not\",\n                        \"hasn't\": \"has not\",\n                        \"haven't\": \"have not\",\n                        \"hadn't\": \"had not\",\n                        \"won't\": \"will not\",\n                        \"wouldn't\": \"would not\",\n                        \"can't\": \"can not\",\n                        \"cannot\": \"can not\",\n                        \"i'm\": \"i am\",\n                        \"m\": \"am\",\n                        \"i'll\" : \"i will\",\n                        \"its\" : \"it is\",\n                        \"it's\" : \"it is\",\n                        \"'s\" : \" is\",\n                        \"that's\" : \"that is\",\n                        \"weren't\" : \"were not\",\n                        \"\\buu\\b\":\"u\"\n                    } \n\n\n\ndef handle_punctuation(text):\n    text = text.translate(REMOVE_DICT)\n    text = text.translate(ISOLATE_DICT)\n    return text\n\ndef clean_contractions(text, mapping=CONTRACTION_MAPPING):\n    '''\n    Expand contractions\n    '''\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = clean_contractions(x)\n    return x\n\n#----------------------------------- Embedding -------------------------------------#\ndef get_coefs(word, *arr):\n    \"\"\"\n    Get word, word_embedding from a pretrained embedding file\n    \"\"\"\n    return word, np.asarray(arr,dtype='float32')\n\ndef load_embeddings(path):\n    if path.split('.')[-1] in ['txt','vec']: # for original pretrained embedding files (extension .text, .vec)\n        with open(path,'rb') as f:\n            return dict(get_coefs(*line.strip().split(' ')) for line in f)    \n    if path.split('.')[-1] =='pkl': # for pickled pretrained embedding files (extention pkl). Loading pickeled embeddings is faster than texts\n        with open(path,'rb') as f:\n            return pickle.load(f)\n    \n\n\n\"\"\"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index)+1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\"\"\"\n    \ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    nb_words=min([MAX_WORDS,len(word_index.items())])\n    embedding_matrix = np.zeros((nb_words+1, 300))\n    max_features=nb_words\n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define LSTM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.engine.topology import Layer\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\nfrom keras.layers import Wrapper\n\nfrom keras.engine.topology import Layer\ndef dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List, Tuple\n\n\ndef channel_normalization(x):\n    # type: (Layer) -> Layer\n    \"\"\" Normalize a layer to the maximum activation\n    This keeps a layers values between zero and one.\n    It helps with relu's unbounded activation\n    Args:\n        x: The layer to normalize\n    Returns:\n        A maximal normalized layer\n    \"\"\"\n    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n    out = x / max_values\n    return out\n\n\ndef wave_net_activation(x):\n    # type: (Layer) -> Layer\n    \"\"\"This method defines the activation used for WaveNet\n    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n    Args:\n        x: The layer we want to apply the activation to\n    Returns:\n        A new layer with the wavenet activation applied\n    \"\"\"\n    tanh_out = Activation('tanh')(x)\n    sigm_out = Activation('sigmoid')(x)\n    return keras.layers.multiply([tanh_out, sigm_out])\n\n\ndef residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n    # type: (Layer, int, int, str, int, int, float, str) -> Tuple[Layer, Layer]\n    \"\"\"Defines the residual block for the WaveNet TCN\n    Args:\n        x: The previous layer in the model\n        s: The stack index i.e. which stack in the overall TCN\n        i: The dilation power of 2 we are using for this residual block\n        activation: The name of the type of activation to use\n        nb_filters: The number of convolutional filters to use in this block\n        kernel_size: The size of the convolutional kernel\n        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n        name: Name of the model. Useful when having multiple TCN.\n    Returns:\n        A tuple where the first element is the residual model layer, and the second\n        is the skip connection.\n    \"\"\"\n\n    original_x = x\n    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n                  dilation_rate=i, padding=padding,\n                  name=name + '_dilated_conv_%d_tanh_s%d' % (i, s))(x)\n    if activation == 'norm_relu':\n        x = Activation('relu')(conv)\n        x = Lambda(channel_normalization)(x)\n    elif activation == 'wavenet':\n        x = wave_net_activation(conv)\n    else:\n        x = Activation(activation)(conv)\n\n    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n\n    # 1x1 conv.\n    x = Convolution1D(nb_filters, 1, padding='same')(x)\n    res_x = keras.layers.add([original_x, x])\n    return res_x, x\n\n\ndef process_dilations(dilations):\n    def is_power_of_two(num):\n        return num != 0 and ((num & (num - 1)) == 0)\n\n    if all([is_power_of_two(i) for i in dilations]):\n        return dilations\n\n    else:\n        new_dilations = [2 ** i for i in dilations]\n        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n        return new_dilations\n\n\nclass TCN(Layer):\n    \"\"\"Creates a TCN layer.\n        Args:\n            input_layer: A tensor of shape (batch_size, timesteps, input_dim).\n            nb_filters: The number of filters to use in the convolutional layers.\n            kernel_size: The size of the kernel to use in each convolutional layer.\n            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n            nb_stacks : The number of stacks of residual blocks to use.\n            activation: The activations to use (norm_relu, wavenet, relu...).\n            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n            name: Name of the model. Useful when having multiple TCN.\n        Returns:\n            A TCN layer.\n        \"\"\"\n\n    def __init__(self,\n                 nb_filters=64,\n                 kernel_size=2,\n                 nb_stacks=1,\n                 dilations=None,\n                 activation='norm_relu',\n                 padding='causal',\n                 use_skip_connections=True,\n                 dropout_rate=0.0,\n                 return_sequences=True,\n                 name='tcn'):\n        super().__init__()\n        self.name = name\n        self.return_sequences = return_sequences\n        self.dropout_rate = dropout_rate\n        self.use_skip_connections = use_skip_connections\n        self.activation = activation\n        self.dilations = dilations\n        self.nb_stacks = nb_stacks\n        self.kernel_size = kernel_size\n        self.nb_filters = nb_filters\n        self.padding = padding\n\n        # backwards incompatibility warning.\n        # o = tcn.TCN(i, return_sequences=False) =>\n        # o = tcn.TCN(return_sequences=False)(i)\n\n        if padding != 'causal' and padding != 'same':\n            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n\n        if not isinstance(nb_filters, int):\n            print('An interface change occurred after the version 2.1.2.')\n            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n            raise Exception()\n\n    def __call__(self, inputs):\n        if self.dilations is None:\n            self.dilations = [1, 2, 4, 8, 16, 32]\n        x = inputs\n        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n        skip_connections = []\n        for s in range(self.nb_stacks):\n            for i in self.dilations:\n                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n                skip_connections.append(skip_out)\n        if self.use_skip_connections:\n            x = keras.layers.add(skip_connections)\n        x = Activation('relu')(x)\n\n        if not self.return_sequences:\n            output_slice_index = -1\n            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n        return x\n\n\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n    \n    def get_config(self):\n        config = {'dim_capsule': self.dim_capsule}\n        base_config = super(Capsule, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n\n\ndef build_model(embedding_matrix, num_aux_targets, loss_weight):\n    \n    input_nums=Input(shape=(len(num_cols),))\n    real=Dense(48, activation='relu')(input_nums)\n    real=Dropout(0.2)(real)\n    real = BatchNormalization()(real)\n    \n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False, input_length=MAX_LEN)(words)\n    x = SpatialDropout1D(0.2)(x)\n    \n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n    atten_2 = AttentionWithContext()(x)\n    \n    capsule_2 = Capsule(num_capsule=10, dim_capsule=6, routings=4, share_weights=True)(x)\n    capsule_2 = Flatten()(capsule_2)\n    \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n\n    hidden = concatenate([avg_pool,\n                        max_pool,\n                        atten_2, \n                        capsule_2,\n                        real\n                        ])\n    \n    hidden = concatenate([Dense(4 * LSTM_UNITS, activation='relu')(hidden), hidden])\n    hidden = concatenate([Dense(4 * LSTM_UNITS, activation='relu')(hidden), hidden])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[\n                            words\n                            ,input_nums\n                            ], outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], optimizer='adam', loss_weights=[loss_weight, 1.0])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npositive_emojis = set([\n\":‑)\",\":)\",\":-]\",\":]\",\":-3\",\":3\",\":->\",\":>\",\"8-)\",\"8)\",\":-}\",\":}\",\":o)\",\":c)\",\":^)\",\"=]\",\"=)\",\":‑D\",\":D\",\"8‑D\",\"8D\",\n\"x‑D\",\"xD\",\"X‑D\",\"XD\",\"=D\",\"=3\",\"B^D\",\":-))\",\";‑)\",\";)\",\"*-)\",\"*)\",\";‑]\",\";]\",\";^)\",\":‑,\",\";D\",\":‑P\",\":P\",\"X‑P\",\"XP\",\n\"x‑p\",\"xp\",\":‑p\",\":p\",\":‑Þ\",\":Þ\",\":‑þ\",\":þ\",\":‑b\",\":b\",\"d:\",\"=p\",\">:P\", \":'‑)\", \":')\",  \":-*\", \":*\", \":×\"\n])\n\nnegative_emojis = set([\n\":‑(\",\":(\",\":‑c\",\":c\",\":‑<\",\":<\",\":‑[\",\":[\",\":-||\",\">:[\",\":{\",\":@\",\">:(\",\"D‑':\",\"D:<\",\"D:\",\"D8\",\"D;\",\"D=\",\"DX\",\":‑/\",\n\":/\",\":‑.\",'>:\\\\', \">:/\", \":\\\\\", \"=/\" ,\"=\\\\\", \":L\", \"=L\",\":S\",\":‑|\",\":|\",\"|‑O\",\"<:‑|\"\n])\n\ntrain = train_df.copy()\ndel(train_df) \ngc.collect()\n\ntest = test_df.copy()\ndel(test_df)\ngc.collect()\n\ntrain['comment_text'] = train['comment_text'].str.replace(\"’‘´`\", \"'\")\n#train['comment_text'] = train['comment_text'].apply(lambda x:' '.join([clean_contractions(st, bad_case_words) for st in str(x).split()]))\ngc.collect()\ntest['comment_text'] = test['comment_text'].str.replace(\"’‘´`\", \"'\")\n#test['comment_text'] = test['comment_text'].apply(lambda x:' '.join([clean_contractions(st, bad_case_words) for st in str(x).split()]))\ngc.collect()\n\ndef has_digit(x):\n    try:\n        return len([int(s) for s in set(x) if s.isdigit()])\n    except:\n        return 0\n        \ndef get_num_features(data):\n    data['total_length'] = data['comment_text'].apply(len)\n    data['capitals'] = data['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)\n    data['num_exclamation_marks'] = data['comment_text'].apply(lambda comment: comment.count('!'))\n    data['num_question_marks'] = data['comment_text'].apply(lambda comment: comment.count('?'))\n    data['num_punctuation'] = data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n    data['num_symbols'] = data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'))\n    data['num_words'] = data['comment_text'].apply(lambda comment: len(comment.split()))\n    data['num_unique_words'] = data['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    data['words_vs_unique'] = data['num_unique_words'] / data['num_words']\n    data['num_pos_smilies'] = data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in set(positive_emojis)))\n    data['num_neg_smilies'] = data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in set(negative_emojis)))\n    data['Vow'] = data.comment_text.apply(lambda x: len(re.findall('[aeyuio]', x, re.IGNORECASE)))\n    data['mean_char_cnt']=data.comment_text.apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    data[\"has_digit\"]=data.comment_text.map(lambda x: has_digit(x))\n    data[\"trump_cnt\"]=data.comment_text.map(lambda x: str(x).lower().count('trump'))\n    return data\n\ntrain=get_num_features(train)\ngc.collect()\n\ntest=get_num_features(test)\ngc.collect()\n\nt_duble=train.comment_text.value_counts().to_dict()\nte_duble=test.comment_text.value_counts().to_dict()\ntrain['is_dublicate']=train.comment_text.map(lambda x:1 if t_duble[x]>1 else 0)\ntest['is_dublicate']=test.comment_text.map(lambda x:1 if te_duble[x]>1 else 0)\ngc.collect()\n\ndel(t_duble, te_duble)\ngc.collect()\n\nnum_cols=['mean_char_cnt','num_neg_smilies','num_pos_smilies','words_vs_unique',\n        'num_unique_words', 'num_words', 'num_symbols', 'num_punctuation',\n        'Vow', 'has_digit', 'caps_vs_length', 'capitals', 'total_length',\n       'is_dublicate', 'num_question_marks', 'num_exclamation_marks', 'trump_cnt']\n\ntrain['comment_text'] = train['comment_text'].apply(lambda x:preprocess(x))\ntest['comment_text'] = test['comment_text'].apply(lambda x:preprocess(x))\ngc.collect()\n\ntrain['comment_text']=train['comment_text'].map(lambda x: 'clear message' if x=='' else x)\ntest['comment_text']=test['comment_text'].map(lambda x: 'clear message' if x=='' else x)\n\ngc.collect()\n\ntrain['comment_text']=train.comment_text.str.strip().str.replace('  ', ' ')\ntest['comment_text']=test.comment_text.str.strip().str.replace('  ', ' ')\n\ngc.collect()\n\nx_train = train['comment_text']\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Overall\nweights = np.ones((len(x_train),)) / 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\nx_test = test['comment_text']\n\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False, num_words=MAX_WORDS)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\nx_tr_real=train[num_cols]\nx_te_real=test[num_cols]\n\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_PATHS], axis=-1)\n\ndel(identity_columns, weights, tokenizer, train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(['comment_text'], axis=1, inplace=True)\ngc.collect()\ntest.drop(num_cols, axis=1, inplace=True)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Model training\n\n* 2 models will be trained (NUM_MODELS=2)\n* Make predictions at the end of each epoch\n* Weighted averaging epoch predictions\n* Weights = 2 ** epoch"},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncheckpoint_predictions = []\nweights = []\nNUM_MODELS = 2\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1], loss_weight)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            [\n                x_train \n            ,x_tr_real\n            ],\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1,\n            callbacks = [\n                LearningRateScheduler(lambda _: 1e-3*(0.55**global_epoch)) # Decayed learning rate 0.55 best\n            ]\n        )\n        checkpoint_predictions.append(model.predict([x_test\n                                                        , x_te_real\n                                                        ], batch_size=2048)[0].flatten())\n        gc.collect()\n        weights.append(2 ** global_epoch)\n    del model\n    cuda.select_device(0)\n    cuda.close()\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\nlstm_submission = pd.DataFrame.from_dict({\n    'id': test.id,\n    'prediction': predictions\n})\n#lstm_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n'id': test['id'],\n'prediction': lstm_submission['prediction'].rank(pct=True)*0.3 + bert_submission['prediction'].rank(pct=True)*0.7})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}