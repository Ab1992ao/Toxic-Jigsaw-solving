{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pretrained BERT models\nhttps://www.kaggle.com/sagol79/bert-inference-2/downloads/bert-inference-2.zip/4"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom numba import cuda\n\nimport sys\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)\nimport torch.utils.data\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nimport gc\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\n\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\nMAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nBATCH_SIZE = 32\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nLARGE_BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pretrained BERT models - Google's pretrained BERT model\nBERT_SMALL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nBERT_LARGE_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\n\n# JIGSAW fine-tuned BERT models\nJIGSAW_BERT_SMALL_MODEL_PATH = '../input/bert-inference-2/bert_2/bert_pytorch.bin'\nJIGSAW_BERT_LARGE_MODEL_PATH = '../input/pretrained-b-j/jigsaw-bert-large-uncased-len-220-fp16/epoch-1/pytorch_model.bin'\nJIGSAW_BERT_SMALL_JSON_PATH = '../input/bert-inference-2/bert_2/bert_config.json'\nJIGSAW_BERT_LARGE_JSON_PATH = '../input/pretrained-b-j/jigsaw-bert-large-uncased-len-220-fp16/epoch-1/config.json'\nNUM_BERT_MODELS = 1 #2\nINFER_BATCH_SIZE = 64\n\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\ntest_preds = np.zeros((test_df.shape[0],NUM_BERT_MODELS))\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\n\nprint(\"Predicting BERT large model......\")\n\n# Prepare data\nbert_config = BertConfig(JIGSAW_BERT_LARGE_JSON_PATH)\ntokenizer = BertTokenizer.from_pretrained(BERT_LARGE_PATH, cache_dir=None,do_lower_case=True)\nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))      \n\n# Load fine-tuned BERT model\ngc.collect()\nmodel = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(JIGSAW_BERT_LARGE_MODEL_PATH))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\n# Predicting\nmodel_preds = np.zeros((len(X_test)))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=INFER_BATCH_SIZE, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        model_preds[i * INFER_BATCH_SIZE:(i + 1) * INFER_BATCH_SIZE] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_preds[:,0] = torch.sigmoid(torch.tensor(model_preds)).numpy().ravel()\ndel model\ngc.collect()\n\n\"\"\" print(\"Predicting BERT small model......\")\nbert_config = BertConfig(JIGSAW_BERT_SMALL_JSON_PATH)\ntokenizer = BertTokenizer.from_pretrained(BERT_SMALL_PATH, cache_dir=None,do_lower_case=True)\nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))      \n\n# # # Load fine-tuned BERT model\nmodel = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(JIGSAW_BERT_SMALL_MODEL_PATH))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\n# Predicting\nmodel_preds = np.zeros((len(X_test)))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=INFER_BATCH_SIZE, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        model_preds[i * INFER_BATCH_SIZE:(i + 1) * INFER_BATCH_SIZE] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_preds[:,1] = torch.sigmoid(torch.tensor(model_preds)).numpy().ravel()\n\ndel model\ngc.collect()\"\"\"\n\n# Sub-model prediction\nbert_submission = pd.DataFrame.from_dict({\n'id': test_df['id'],\n'prediction': test_preds.mean(axis=1)})\nbert_submission.to_csv('bert_submission.csv', index=False)\ncuda.select_device(0)\ncuda.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Credits**\nThis notebook was mainly inspired by the following awesome kernel scripts:\n\nhttps://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n\nhttps://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage\n\nhttps://www.kaggle.com/shubham505/apply-by-simple-bilstm\n\n\n\n# Preparations\n\n## Datasets\nYou will need to add the following Kaggle dataset for pickled pretrained embeddings\n\nhttps://www.kaggle.com/chriscc/pickled-word-embedding\n\nhttps://www.kaggle.com/firewalkor/pretrained-b-j\n\n## Import packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport re\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nimport pickle\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import add\nfrom keras.layers import Input,Lambda, CuDNNLSTM, Convolution1D, GlobalMaxPooling1D ,GlobalAveragePooling1D, CuDNNGRU\nfrom keras.layers.embeddings import Embedding\nfrom keras import optimizers\nfrom keras.layers import Dense, concatenate, Activation, Dropout, SpatialDropout1D, Dot, Reshape, Bidirectional, BatchNormalization, Flatten\nfrom keras.models import Model, Sequential\nfrom keras.layers.convolutional import Conv1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.initializers import glorot_normal, orthogonal\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras\nfrom keras.utils import np_utils\nimport gc\nimport keras.backend as K\nfrom keras.optimizers import Adadelta\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn.metrics import f1_score\nfrom keras.losses import binary_crossentropy\nfrom tensorflow import set_random_seed\nfrom keras import backend\nimport tensorflow as tf\nimport string","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Configurations"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_PATHS = ['../input/pickled-word-embedding/crawl-300d-2M.pkl',\n                 '../input/pickled-word-embedding/glove.840B.300d.pkl']\n\n\nNUM_MODELS = 2 # The number of classifiers we want to train \nBATCH_SIZE = 512 # can be tuned\nLSTM_UNITS = 128 # can be tuned\nDENSE_HIDDEN_UNITS = 4*LSTM_UNITS # can betuned\nEPOCHS = 4 # The number of epoches we want to train for each classifier\nMAX_LEN = 220 # can ben tuned\nMAX_WORDS=450000 #best 450k\n\nIDENTITY_COLUMNS = [\n    'transgender', 'female', 'homosexual_gay_or_lesbian', 'muslim', 'hindu',\n    'white', 'black', 'psychiatric_or_mental_illness', 'jewish'\n    ]  \n\nAUX_COLUMNS = ['target', 'severe_toxicity','obscene','identity_attack','insult','threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------------------------- Preprocessing-------------------------------------#\nSYMBOLS_TO_ISOLATE = '.,?!-;*\"‚Ä¶:‚Äî()%#$&_/@Ôºº„Éªœâ+=‚Äù‚Äú[]^‚Äì>\\\\¬∞<~‚Ä¢‚â†‚Ñ¢Àà ä…í‚àû¬ß{}¬∑œÑŒ±‚ù§‚ò∫…°|¬¢‚ÜíÃ∂`‚ù•‚îÅ‚î£‚î´‚îóÔºØ‚ñ∫‚òÖ¬©‚Äï…™‚úî¬Æ\\x96\\x92‚óè¬£‚ô•‚û§¬¥¬π‚òï‚âà√∑‚ô°‚óê‚ïë‚ñ¨‚Ä≤…îÀê‚Ç¨€©€û‚Ä†Œº‚úí‚û•‚ïê‚òÜÀå‚óÑ¬Ω ªœÄŒ¥Œ∑ŒªœÉŒµœÅŒΩ É‚ú¨Ôº≥ÔºµÔº∞Ôº•Ôº≤Ôº©Ôº¥‚òª¬±‚ôç¬µ¬∫¬æ‚úì‚óæÿüÔºé‚¨Ö‚ÑÖ¬ª–í–∞–≤‚ù£‚ãÖ¬ø¬¨‚ô´Ôº£Ôº≠Œ≤‚ñà‚ñì‚ñí‚ñë‚áí‚≠ê‚Ä∫¬°‚ÇÇ‚ÇÉ‚ùß‚ñ∞‚ñî‚óû‚ñÄ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ÜôŒ≥ÃÑ‚Ä≥‚òπ‚û°¬´œÜ‚Öì‚Äû‚úãÔºö¬•Ã≤ÃÖÃÅ‚àô‚Äõ‚óá‚úè‚ñ∑‚ùì‚ùó¬∂ÀöÀôÔºâ—Å–∏ ø‚ú®„ÄÇ…ë\\x80‚óïÔºÅÔºÖ¬Ø‚àíÔ¨ÇÔ¨Å‚ÇÅ¬≤ å¬º‚Å¥‚ÅÑ‚ÇÑ‚å†‚ô≠‚úò‚ï™‚ñ∂‚ò≠‚ú≠‚ô™‚òî‚ò†‚ôÇ‚òÉ‚òé‚úà‚úå‚ú∞‚ùÜ‚òô‚óã‚Ä£‚öìÂπ¥‚àé‚Ñí‚ñ™‚ñô‚òè‚ÖõÔΩÉÔΩÅÔΩì«Ä‚ÑÆ¬∏ÔΩó‚Äö‚àº‚Äñ‚Ñ≥‚ùÑ‚Üê‚òº‚ãÜ í‚äÇ„ÄÅ‚Öî¬®Õ°‡πè‚öæ‚öΩŒ¶√óŒ∏Ôø¶ÔºüÔºà‚ÑÉ‚è©‚òÆ‚ö†Êúà‚úä‚ùå‚≠ï‚ñ∏‚ñ†‚áå‚òê‚òë‚ö°‚òÑ«´‚ï≠‚à©‚ïÆÔºå‰æãÔºû ï…êÃ£Œî‚ÇÄ‚úû‚îà‚ï±‚ï≤‚ñè‚ñï‚îÉ‚ï∞‚ñä‚ñã‚ïØ‚î≥‚îä‚â•‚òí‚Üë‚òù…π‚úÖ‚òõ‚ô©‚òûÔº°Ôº™Ôº¢‚óî‚ó°‚Üì‚ôÄ‚¨ÜÃ±‚Ñè\\x91‚†ÄÀ§‚ïö‚Ü∫‚á§‚àè‚úæ‚ó¶‚ô¨¬≥„ÅÆÔΩúÔºè‚àµ‚à¥‚àöŒ©¬§‚òú‚ñ≤‚Ü≥‚ñ´‚Äø‚¨á‚úßÔΩèÔΩñÔΩçÔºçÔºíÔºêÔºòÔºá‚Ä∞‚â§‚àïÀÜ‚öú‚òÅ'\nSYMBOLS_TO_REMOVE = '\\nüçï\\rüêµ\\xa0\\ue014\\t\\uf818\\uf04a\\xadüò¢üê∂Ô∏è\\uf0e0üòúüòéüëä\\u200b\\u200eüòÅÿπÿØŸàŸäŸáÿµŸÇÿ£ŸÜÿßÿÆŸÑŸâÿ®ŸÖÿ∫ÿ±üòçüíñüíµ–ïüëéüòÄüòÇ\\u202a\\u202cüî•üòÑüèªüí•·¥ç è Ä·¥á…¥·¥Ö·¥è·¥Ä·¥ã ú·¥ú ü·¥õ·¥Ñ·¥ò ô“ì·¥ä·¥°…¢üòãüëè◊©◊ú◊ï◊ù◊ë◊ôüò±‚Äº\\x81„Ç®„É≥„Ç∏ÊïÖÈöú\\u2009üöå·¥µÕûüåüüòäüò≥üòßüôÄüòêüòï\\u200füëçüòÆüòÉüòò◊ê◊¢◊õ◊óüí©üíØ‚õΩüöÑüèº‡Æúüòñ·¥†üö≤‚Äêüòüüòàüí™üôèüéØüåπüòáüíîüò°\\x7füëå·ºê·Ω∂ŒÆŒπ·Ω≤Œ∫·ºÄŒØ·øÉ·º¥ŒæüôÑÔº®üò†\\ufeff\\u2028üòâüò§‚õ∫üôÇ\\u3000ÿ™ÿ≠ŸÉÿ≥ÿ©üëÆüíôŸÅÿ≤ÿ∑üòèüçæüéâüòû\\u2008üèæüòÖüò≠üëªüò•üòîüòìüèΩüéÜüçªüçΩüé∂üå∫ü§îüò™\\x08‚Äëüê∞üêáüê±üôÜüò®üôÉüíïùòäùò¶ùò≥ùò¢ùòµùò∞ùò§ùò∫ùò¥ùò™ùòßùòÆùò£üíóüíöÂú∞ÁçÑË∞∑—É–ª–∫–Ω–ü–æ–ê–ùüêæüêïüòÜ◊îüîóüöΩÊ≠åËàû‰ºéüôàüò¥üèøü§óüá∫üá∏–ºœÖ—Ç—ï‚§µüèÜüéÉüò©\\u200aüå†üêüüí´üí∞üíé—ç–ø—Ä–¥\\x95üñêüôÖ‚õ≤üç∞ü§êüëÜüôå\\u2002üíõüôÅüëÄüôäüôâ\\u2004À¢·µí ≥ ∏·¥º·¥∑·¥∫ ∑·µó ∞·µâ·µò\\x13üö¨ü§ì\\ue602üòµŒ¨ŒøœåœÇŒ≠·Ω∏◊™◊û◊ì◊£◊†◊®◊ö◊¶◊òüòíÕùüÜïüëÖüë•üëÑüîÑüî§üëâüë§üë∂üë≤üîõüéì\\uf0b7\\uf04c\\x9f\\x10ÊàêÈÉΩüò£‚è∫üòåü§ëüåèüòØ–µ—Öüò≤·º∏·æ∂·ΩÅüíûüöìüîîüìöüèÄüëê\\u202düí§üçá\\ue613Â∞èÂúüË±Üüè°‚ùî‚Åâ\\u202füë†„Äã‡§ï‡§∞‡•ç‡§Æ‡§æüáπüáºüå∏Ëî°Ëã±Êñáüåûüé≤„É¨„ÇØ„Çµ„ÇπüòõÂ§ñÂõΩ‰∫∫ÂÖ≥Á≥ª–°–±üíãüíÄüéÑüíúü§¢ŸéŸê—å—ã–≥—è‰∏çÊòØ\\x9c\\x9düóë\\u2005üíÉüì£üëø‡ºº„Å§‡ºΩüò∞·∏∑–ó–∑‚ñ±—ÜÔøºü§£ÂçñÊ∏©Âì•ÂçéËÆÆ‰ºö‰∏ãÈôç‰Ω†Â§±ÂéªÊâÄÊúâÁöÑÈí±Âä†ÊãøÂ§ßÂùèÁ®éÈ™óÂ≠êüêù„ÉÑüéÖ\\x85üç∫ÿ¢ÿ•ÿ¥ÿ°üéµüåéÕü·ºîÊ≤πÂà´ÂÖãü§°ü§•üò¨ü§ß–π\\u2003üöÄü§¥ ≤—à—á–ò–û–†–§–î–Ø–ú—é–∂üòùüñë·ΩêœçœçÁâπÊÆä‰ΩúÊà¶Áæ§—âüí®ÂúÜÊòéÂõ≠◊ß‚Ñêüèàüò∫üåç‚èè·ªáüçîüêÆüçÅüçÜüçëüåÆüåØü§¶\\u200dùìíùì≤ùìøùìµÏïàÏòÅÌïòÏÑ∏Ïöî–ñ—ô–ö—õüçÄüò´ü§§·ø¶ÊàëÂá∫ÁîüÂú®‰∫ÜÂèØ‰ª•ËØ¥ÊôÆÈÄöËØùÊ±âËØ≠Â•ΩÊûÅüéºüï∫üç∏ü•ÇüóΩüéáüéäüÜòü§†üë©üñíüö™Â§©‰∏ÄÂÆ∂‚ö≤\\u2006‚ö≠‚öÜ‚¨≠‚¨Ø‚èñÊñ∞‚úÄ‚ïåüá´üá∑üá©üá™üáÆüá¨üáßüò∑üá®üá¶–•–®üåê\\x1fÊùÄÈ∏°ÁªôÁå¥Áúã Åùó™ùóµùó≤ùóªùòÜùóºùòÇùóøùóÆùóπùó∂ùòáùóØùòÅùó∞ùòÄùòÖùóΩùòÑùó±üì∫œñ\\u2000“Ø’Ω·¥¶·é•“ªÕ∫\\u2007’∞\\u2001…©ÔΩôÔΩÖ‡µ¶ÔΩå∆ΩÔΩàùêìùê°ùêûùê´ùêÆùêùùêöùêÉùêúùê©ùê≠ùê¢ùê®ùêß∆Ñ·¥®◊ü·ëØ‡ªêŒ§·èß‡Ø¶–Ü·¥ë‹Åùê¨ùê∞ùê≤ùêõùê¶ùêØùêëùêôùê£ùêáùêÇùêòùüé‘ú–¢·óû‡±¶„Äî·é´ùê≥ùêîùê±ùüîùüìùêÖüêãÔ¨Éüíòüíì—ëùò•ùòØùò∂üíêüåãüåÑüåÖùô¨ùôñùô®ùô§ùô£ùô°ùôÆùôòùô†ùôöùôôùôúùôßùô•ùô©ùô™ùôóùôûùôùùôõüë∫üê∑‚ÑãùêÄùê•ùê™üö∂ùô¢·ºπü§òÕ¶üí∏ÿ¨Ìå®Ìã∞Ôº∑ùôá·µªüëÇüëÉ…úüé´\\uf0a7–ë–£—ñüö¢üöÇ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä·øÜüèÉùì¨ùìªùì¥ùìÆùìΩùìº‚òòÔ¥æÃØÔ¥ø‚ÇΩ\\ue807ùëªùíÜùíçùíïùíâùíìùíñùíÇùíèùíÖùíîùíéùíóùíäüëΩüòô\\u200c–õ‚Äíüéæüëπ‚éåüèí‚õ∏ÂÖ¨ÂØìÂÖªÂÆ†Áâ©ÂêóüèÑüêÄüöëü§∑ÊìçÁæéùíëùíöùíêùë¥ü§ôüêíÊ¨¢ËøéÊù•Âà∞ÈòøÊãâÊñØ◊°◊§ùô´üêàùíåùôäùô≠ùôÜùôãùôçùòºùôÖÔ∑ªü¶ÑÂ∑®Êî∂Ëµ¢ÂæóÁôΩÈ¨ºÊÑ§ÊÄíË¶Å‰π∞È¢ù·∫Ωüöóüê≥ùüèùêüùüñùüëùüïùíÑùüóùê†ùôÑùôÉüëáÈîüÊñ§Êã∑ùó¢ùü≥ùü±ùü¨‚¶Å„Éû„É´„Éè„Éã„ÉÅ„É≠Ê†™ÂºèÁ§æ‚õ∑ÌïúÍµ≠Ïñ¥„Ñ∏„ÖìÎãàÕú ñùòøùôî‚Çµùí©‚ÑØùíæùìÅùí∂ùìâùìáùìäùìÉùìàùìÖ‚Ñ¥ùíªùíΩùìÄùìåùí∏ùìéùôèŒ∂ùôüùòÉùó∫ùüÆùü≠ùüØùü≤üëãü¶äÂ§ö‰º¶üêΩüéªüéπ‚õìüèπüç∑ü¶Ü‰∏∫Âíå‰∏≠ÂèãË∞äÁ•ùË¥∫‰∏éÂÖ∂ÊÉ≥Ë±°ÂØπÊ≥ïÂ¶ÇÁõ¥Êé•ÈóÆÁî®Ëá™Â∑±ÁåúÊú¨‰º†ÊïôÂ£´Ê≤°ÁßØÂîØËÆ§ËØÜÂü∫Áù£ÂæíÊõæÁªèËÆ©Áõ∏‰ø°ËÄ∂Á®£Â§çÊ¥ªÊ≠ªÊÄ™‰ªñ‰ΩÜÂΩì‰ª¨ËÅä‰∫õÊîøÊ≤ªÈ¢òÊó∂ÂÄôÊàòËÉúÂõ†Âú£ÊääÂÖ®Â†ÇÁªìÂ©öÂ≠©ÊÅêÊÉß‰∏îÊ†óË∞ìËøôÊ†∑Ëøò‚ôæüé∏ü§ïü§í‚õëüéÅÊâπÂà§Ê£ÄËÆ®üèùü¶Åüôãüò∂Ï•êÏä§ÌÉ±Ìä∏Î§ºÎèÑÏÑùÏú†Í∞ÄÍ≤©Ïù∏ÏÉÅÏù¥Í≤ΩÏ†úÌô©ÏùÑÎ†µÍ≤åÎßåÎì§ÏßÄÏïäÎ°ùÏûòÍ¥ÄÎ¶¨Ìï¥ÏïºÌï©Îã§Ï∫êÎÇòÏóêÏÑúÎåÄÎßàÏ¥àÏôÄÌôîÏïΩÍ∏àÏùòÌíàÎü∞ÏÑ±Î∂ÑÍ∞àÎïåÎäîÎ∞òÎìúÏãúÌóàÎêúÏÇ¨Ïö©üî´üëÅÂá∏·Ω∞üí≤üóØùôà·ºåùíáùíàùíòùíÉùë¨ùë∂ùïæùñôùñóùñÜùñéùñåùñçùñïùñäùñîùñëùñâùñìùñêùñúùñûùñöùñáùïøùñòùñÑùñõùñíùñãùñÇùï¥ùñüùñàùï∏üëëüöøüí°Áü•ÂΩºÁôæ\\uf005ùôÄùíõùë≤ùë≥ùëæùíãùüíüò¶ùôíùòæùòΩüèêùò©ùò®·Ωº·πëùë±ùëπùë´ùëµùë™üá∞üáµüëæ·ìá·íß·î≠·êÉ·êß·ê¶·ë≥·ê®·ìÉ·ìÇ·ë≤·ê∏·ë≠·ëé·ìÄ·ê£üêÑüéàüî®üêéü§ûüê∏üíüüé∞üåùüõ≥ÁÇπÂáªÊü•Áâàüç≠ùë•ùë¶ùëßÔºÆÔºßüë£\\uf020„Å£üèâ—Ñüí≠üé•Œûüê¥üë®ü§≥ü¶ç\\x0büç©ùëØùííüòóùüêüèÇüë≥üçóüïâüê≤⁄Ü€åùëÆùóïùó¥üçíÍú•‚≤£‚≤èüêë‚è∞ÈâÑ„É™‰∫ã‰ª∂—óüíä„Äå„Äç\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ÁáªË£Ω„Ç∑ËôöÂÅΩÂ±ÅÁêÜÂ±à–ìùë©ùë∞ùíÄùë∫üå§ùó≥ùóúùóôùó¶ùóßüçä·Ω∫·ºà·º°œá·øñŒõ‚§èüá≥ùíôœà’Å’¥’•’º’°’µ’´’∂÷Ä÷Ç’§’±ÂÜ¨Ëá≥·ΩÄùíÅüîπü§öüçéùë∑üêÇüíÖùò¨ùò±ùò∏ùò∑ùòêùò≠ùòìùòñùòπùò≤ùò´⁄©Œíœéüí¢ŒúŒüŒùŒëŒïüá±‚ô≤ùùà‚Ü¥üíí‚äò»ªüö¥üñïüñ§ü•òüìçüëà‚ûïüö´üé®üåëüêªùêéùêçùêäùë≠ü§ñüééüòºüï∑ÔΩáÔΩíÔΩéÔΩîÔΩâÔΩÑÔΩïÔΩÜÔΩÇÔΩãùü∞üá¥üá≠üáªüá≤ùóûùó≠ùóòùó§üëºüìâüçüüç¶üåàüî≠„Ääüêäüêç\\uf10a·Éö⁄°üê¶\\U0001f92f\\U0001f92aüê°üí≥·º±üôáùó∏ùóüùó†ùó∑ü•ú„Åï„Çà„ÅÜ„Å™„Çâüîº'\n\nSYMBOLS_TO_REMOVE=set(SYMBOLS_TO_REMOVE)\nSYMBOLS_TO_ISOLATE=set(SYMBOLS_TO_ISOLATE)\n\nISOLATE_DICT = {ord(c):f' {c} ' for c in SYMBOLS_TO_ISOLATE}\nREMOVE_DICT = {ord(c):f'' for c in SYMBOLS_TO_REMOVE}\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n‚Äú‚Äù‚Äô\\'‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî'\n\nCHARS_TO_REMOVE=set(CHARS_TO_REMOVE)\n\nCONTRACTION_MAPPING ={\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n                        \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\",\n                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n                       \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n                       \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n                       \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n                       \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n                       \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n                       \"why've\": \"why have\",\"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n                        \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                        \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                       \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\"yay!\": \" good \",\n                        \"yay\": \" good \",\n                        \"yaay\": \" good \",\n                        \"yaaay\": \" good \",\n                        \"yaaaay\": \" good \",\n                        \"yaaaaay\": \" good \",\n                        \":/\": \" bad \",\n                        \":&gt;\": \" sad \",\n                        \":')\": \" sad \",\n                        \":-(\": \" frown \",\n                        \":(\": \" frown \",\n                        \":s\": \" frown \",\n                        \":-s\": \" frown \",\n                        \"&lt;3\": \" heart \",\n                        \":d\": \" smile \",\n                        \":p\": \" smile \",\n                        \":dd\": \" smile \",\n                        \"8)\": \" smile \",\n                        \":-)\": \" smile \",\n                        \":)\": \" smile \",\n                        \";)\": \" smile \",\n                        \"(-:\": \" smile \",\n                        \"(:\": \" smile \",\n                        \":/\": \" worry \",\n                        \":&gt;\": \" angry \",\n                        \":')\": \" sad \",\n                        \":-(\": \" sad \",\n                        \":(\": \" sad \",\n                        \":s\": \" sad \",\n                        \":-s\": \" sad \",\n                        r\"\\br\\b\": \"are\",\n                        r\"\\bu\\b\": \"you\",\n                        r\"\\bhaha\\b\": \"ha\",\n                        r\"\\bhaha\\b\": \"oh\",\n                        r\"\\bhh\\b\": \"h\",\n                        r\"\\bdon't\\b\": \"do not\",\n                        r\"\\bdoesn't\\b\": \"does not\",\n                        r\"\\bdidn't\\b\": \"did not\",\n                        r\"\\bhasn't\\b\": \"has not\",\n                        r\"\\bhaven't\\b\": \"have not\",\n                        r\"\\bhadn't\\b\": \"had not\",\n                        r\"\\bwon't\\b\": \"will not\",\n                        r\"\\bwouldn't\\b\": \"would not\",\n                        r\"\\bcan't\\b\": \"can not\",\n                        r\"\\bcannot\\b\": \"can not\",\n                        r\"\\bi'm\\b\": \"i am\",\n                        \"m\": \"am\",\n                        \"r\": \"are\",\n                        \"u\": \"you\",\n                        \"haha\": \"ha\",\n                        \"hahaha\": \"ha\",\n                        \"don't\": \"do not\",\n                        \"doesn't\": \"does not\",\n                        \"didn't\": \"did not\",\n                        \"hasn't\": \"has not\",\n                        \"haven't\": \"have not\",\n                        \"hadn't\": \"had not\",\n                        \"won't\": \"will not\",\n                        \"wouldn't\": \"would not\",\n                        \"can't\": \"can not\",\n                        \"cannot\": \"can not\",\n                        \"i'm\": \"i am\",\n                        \"m\": \"am\",\n                        \"i'll\" : \"i will\",\n                        \"its\" : \"it is\",\n                        \"it's\" : \"it is\",\n                        \"'s\" : \" is\",\n                        \"that's\" : \"that is\",\n                        \"weren't\" : \"were not\",\n                        \"\\buu\\b\":\"u\"\n                    } \n\n\n\ndef handle_punctuation(text):\n    text = text.translate(REMOVE_DICT)\n    text = text.translate(ISOLATE_DICT)\n    return text\n\ndef clean_contractions(text, mapping=CONTRACTION_MAPPING):\n    '''\n    Expand contractions\n    '''\n    specials = [\"‚Äô\", \"‚Äò\", \"¬¥\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = clean_contractions(x)\n    return x\n\n#----------------------------------- Embedding -------------------------------------#\ndef get_coefs(word, *arr):\n    \"\"\"\n    Get word, word_embedding from a pretrained embedding file\n    \"\"\"\n    return word, np.asarray(arr,dtype='float32')\n\ndef load_embeddings(path):\n    if path.split('.')[-1] in ['txt','vec']: # for original pretrained embedding files (extension .text, .vec)\n        with open(path,'rb') as f:\n            return dict(get_coefs(*line.strip().split(' ')) for line in f)    \n    if path.split('.')[-1] =='pkl': # for pickled pretrained embedding files (extention pkl). Loading pickeled embeddings is faster than texts\n        with open(path,'rb') as f:\n            return pickle.load(f)\n    \n\n\n\"\"\"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index)+1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\"\"\"\n    \ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    nb_words=min([MAX_WORDS,len(word_index.items())])\n    embedding_matrix = np.zeros((nb_words+1, 300))\n    max_features=nb_words\n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define LSTM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.engine.topology import Layer\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\nfrom keras.layers import Wrapper\n\nfrom keras.engine.topology import Layer\ndef dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number Œµ to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List, Tuple\n\n\ndef channel_normalization(x):\n    # type: (Layer) -> Layer\n    \"\"\" Normalize a layer to the maximum activation\n    This keeps a layers values between zero and one.\n    It helps with relu's unbounded activation\n    Args:\n        x: The layer to normalize\n    Returns:\n        A maximal normalized layer\n    \"\"\"\n    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n    out = x / max_values\n    return out\n\n\ndef wave_net_activation(x):\n    # type: (Layer) -> Layer\n    \"\"\"This method defines the activation used for WaveNet\n    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n    Args:\n        x: The layer we want to apply the activation to\n    Returns:\n        A new layer with the wavenet activation applied\n    \"\"\"\n    tanh_out = Activation('tanh')(x)\n    sigm_out = Activation('sigmoid')(x)\n    return keras.layers.multiply([tanh_out, sigm_out])\n\n\ndef residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n    # type: (Layer, int, int, str, int, int, float, str) -> Tuple[Layer, Layer]\n    \"\"\"Defines the residual block for the WaveNet TCN\n    Args:\n        x: The previous layer in the model\n        s: The stack index i.e. which stack in the overall TCN\n        i: The dilation power of 2 we are using for this residual block\n        activation: The name of the type of activation to use\n        nb_filters: The number of convolutional filters to use in this block\n        kernel_size: The size of the convolutional kernel\n        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n        name: Name of the model. Useful when having multiple TCN.\n    Returns:\n        A tuple where the first element is the residual model layer, and the second\n        is the skip connection.\n    \"\"\"\n\n    original_x = x\n    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n                  dilation_rate=i, padding=padding,\n                  name=name + '_dilated_conv_%d_tanh_s%d' % (i, s))(x)\n    if activation == 'norm_relu':\n        x = Activation('relu')(conv)\n        x = Lambda(channel_normalization)(x)\n    elif activation == 'wavenet':\n        x = wave_net_activation(conv)\n    else:\n        x = Activation(activation)(conv)\n\n    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n\n    # 1x1 conv.\n    x = Convolution1D(nb_filters, 1, padding='same')(x)\n    res_x = keras.layers.add([original_x, x])\n    return res_x, x\n\n\ndef process_dilations(dilations):\n    def is_power_of_two(num):\n        return num != 0 and ((num & (num - 1)) == 0)\n\n    if all([is_power_of_two(i) for i in dilations]):\n        return dilations\n\n    else:\n        new_dilations = [2 ** i for i in dilations]\n        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n        return new_dilations\n\n\nclass TCN(Layer):\n    \"\"\"Creates a TCN layer.\n        Args:\n            input_layer: A tensor of shape (batch_size, timesteps, input_dim).\n            nb_filters: The number of filters to use in the convolutional layers.\n            kernel_size: The size of the kernel to use in each convolutional layer.\n            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n            nb_stacks : The number of stacks of residual blocks to use.\n            activation: The activations to use (norm_relu, wavenet, relu...).\n            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n            name: Name of the model. Useful when having multiple TCN.\n        Returns:\n            A TCN layer.\n        \"\"\"\n\n    def __init__(self,\n                 nb_filters=64,\n                 kernel_size=2,\n                 nb_stacks=1,\n                 dilations=None,\n                 activation='norm_relu',\n                 padding='causal',\n                 use_skip_connections=True,\n                 dropout_rate=0.0,\n                 return_sequences=True,\n                 name='tcn'):\n        super().__init__()\n        self.name = name\n        self.return_sequences = return_sequences\n        self.dropout_rate = dropout_rate\n        self.use_skip_connections = use_skip_connections\n        self.activation = activation\n        self.dilations = dilations\n        self.nb_stacks = nb_stacks\n        self.kernel_size = kernel_size\n        self.nb_filters = nb_filters\n        self.padding = padding\n\n        # backwards incompatibility warning.\n        # o = tcn.TCN(i, return_sequences=False) =>\n        # o = tcn.TCN(return_sequences=False)(i)\n\n        if padding != 'causal' and padding != 'same':\n            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n\n        if not isinstance(nb_filters, int):\n            print('An interface change occurred after the version 2.1.2.')\n            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n            raise Exception()\n\n    def __call__(self, inputs):\n        if self.dilations is None:\n            self.dilations = [1, 2, 4, 8, 16, 32]\n        x = inputs\n        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n        skip_connections = []\n        for s in range(self.nb_stacks):\n            for i in self.dilations:\n                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n                skip_connections.append(skip_out)\n        if self.use_skip_connections:\n            x = keras.layers.add(skip_connections)\n        x = Activation('relu')(x)\n\n        if not self.return_sequences:\n            output_slice_index = -1\n            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n        return x\n\n\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n    \n    def get_config(self):\n        config = {'dim_capsule': self.dim_capsule}\n        base_config = super(Capsule, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n\n\ndef build_model(embedding_matrix, num_aux_targets, loss_weight):\n    \n    input_nums=Input(shape=(len(num_cols),))\n    real=Dense(48, activation='relu')(input_nums)\n    real=Dropout(0.2)(real)\n    real = BatchNormalization()(real)\n    \n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False, input_length=MAX_LEN)(words)\n    x = SpatialDropout1D(0.2)(x)\n    \n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n    atten_2 = AttentionWithContext()(x)\n    \n    capsule_2 = Capsule(num_capsule=10, dim_capsule=6, routings=4, share_weights=True)(x)\n    capsule_2 = Flatten()(capsule_2)\n    \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n\n    hidden = concatenate([avg_pool,\n                        max_pool,\n                        atten_2, \n                        capsule_2,\n                        real\n                        ])\n    \n    hidden = concatenate([Dense(4 * LSTM_UNITS, activation='relu')(hidden), hidden])\n    hidden = concatenate([Dense(4 * LSTM_UNITS, activation='relu')(hidden), hidden])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[\n                            words\n                            ,input_nums\n                            ], outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], optimizer='adam', loss_weights=[loss_weight, 1.0])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npositive_emojis = set([\n\":‚Äë)\",\":)\",\":-]\",\":]\",\":-3\",\":3\",\":->\",\":>\",\"8-)\",\"8)\",\":-}\",\":}\",\":o)\",\":c)\",\":^)\",\"=]\",\"=)\",\":‚ÄëD\",\":D\",\"8‚ÄëD\",\"8D\",\n\"x‚ÄëD\",\"xD\",\"X‚ÄëD\",\"XD\",\"=D\",\"=3\",\"B^D\",\":-))\",\";‚Äë)\",\";)\",\"*-)\",\"*)\",\";‚Äë]\",\";]\",\";^)\",\":‚Äë,\",\";D\",\":‚ÄëP\",\":P\",\"X‚ÄëP\",\"XP\",\n\"x‚Äëp\",\"xp\",\":‚Äëp\",\":p\",\":‚Äë√û\",\":√û\",\":‚Äë√æ\",\":√æ\",\":‚Äëb\",\":b\",\"d:\",\"=p\",\">:P\", \":'‚Äë)\", \":')\",  \":-*\", \":*\", \":√ó\"\n])\n\nnegative_emojis = set([\n\":‚Äë(\",\":(\",\":‚Äëc\",\":c\",\":‚Äë<\",\":<\",\":‚Äë[\",\":[\",\":-||\",\">:[\",\":{\",\":@\",\">:(\",\"D‚Äë':\",\"D:<\",\"D:\",\"D8\",\"D;\",\"D=\",\"DX\",\":‚Äë/\",\n\":/\",\":‚Äë.\",'>:\\\\', \">:/\", \":\\\\\", \"=/\" ,\"=\\\\\", \":L\", \"=L\",\":S\",\":‚Äë|\",\":|\",\"|‚ÄëO\",\"<:‚Äë|\"\n])\n\ntrain = train_df.copy()\ndel(train_df) \ngc.collect()\n\ntest = test_df.copy()\ndel(test_df)\ngc.collect()\n\ntrain['comment_text'] = train['comment_text'].str.replace(\"‚Äô‚Äò¬¥`\", \"'\")\n#train['comment_text'] = train['comment_text'].apply(lambda x:' '.join([clean_contractions(st, bad_case_words) for st in str(x).split()]))\ngc.collect()\ntest['comment_text'] = test['comment_text'].str.replace(\"‚Äô‚Äò¬¥`\", \"'\")\n#test['comment_text'] = test['comment_text'].apply(lambda x:' '.join([clean_contractions(st, bad_case_words) for st in str(x).split()]))\ngc.collect()\n\ndef has_digit(x):\n    try:\n        return len([int(s) for s in set(x) if s.isdigit()])\n    except:\n        return 0\n        \ndef get_num_features(data):\n    data['total_length'] = data['comment_text'].apply(len)\n    data['capitals'] = data['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)\n    data['num_exclamation_marks'] = data['comment_text'].apply(lambda comment: comment.count('!'))\n    data['num_question_marks'] = data['comment_text'].apply(lambda comment: comment.count('?'))\n    data['num_punctuation'] = data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n    data['num_symbols'] = data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî‚Äì&'))\n    data['num_words'] = data['comment_text'].apply(lambda comment: len(comment.split()))\n    data['num_unique_words'] = data['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    data['words_vs_unique'] = data['num_unique_words'] / data['num_words']\n    data['num_pos_smilies'] = data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in set(positive_emojis)))\n    data['num_neg_smilies'] = data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in set(negative_emojis)))\n    data['Vow'] = data.comment_text.apply(lambda x: len(re.findall('[aeyuio]', x, re.IGNORECASE)))\n    data['mean_char_cnt']=data.comment_text.apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    data[\"has_digit\"]=data.comment_text.map(lambda x: has_digit(x))\n    data[\"trump_cnt\"]=data.comment_text.map(lambda x: str(x).lower().count('trump'))\n    return data\n\ntrain=get_num_features(train)\ngc.collect()\n\ntest=get_num_features(test)\ngc.collect()\n\nt_duble=train.comment_text.value_counts().to_dict()\nte_duble=test.comment_text.value_counts().to_dict()\ntrain['is_dublicate']=train.comment_text.map(lambda x:1 if t_duble[x]>1 else 0)\ntest['is_dublicate']=test.comment_text.map(lambda x:1 if te_duble[x]>1 else 0)\ngc.collect()\n\ndel(t_duble, te_duble)\ngc.collect()\n\nnum_cols=['mean_char_cnt','num_neg_smilies','num_pos_smilies','words_vs_unique',\n        'num_unique_words', 'num_words', 'num_symbols', 'num_punctuation',\n        'Vow', 'has_digit', 'caps_vs_length', 'capitals', 'total_length',\n       'is_dublicate', 'num_question_marks', 'num_exclamation_marks', 'trump_cnt']\n\ntrain['comment_text'] = train['comment_text'].apply(lambda x:preprocess(x))\ntest['comment_text'] = test['comment_text'].apply(lambda x:preprocess(x))\ngc.collect()\n\ntrain['comment_text']=train['comment_text'].map(lambda x: 'clear message' if x=='' else x)\ntest['comment_text']=test['comment_text'].map(lambda x: 'clear message' if x=='' else x)\n\ngc.collect()\n\ntrain['comment_text']=train.comment_text.str.strip().str.replace('  ', ' ')\ntest['comment_text']=test.comment_text.str.strip().str.replace('  ', ' ')\n\ngc.collect()\n\nx_train = train['comment_text']\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Overall\nweights = np.ones((len(x_train),)) / 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\nx_test = test['comment_text']\n\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False, num_words=MAX_WORDS)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\nx_tr_real=train[num_cols]\nx_te_real=test[num_cols]\n\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_PATHS], axis=-1)\n\ndel(identity_columns, weights, tokenizer, train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(['comment_text'], axis=1, inplace=True)\ngc.collect()\ntest.drop(num_cols, axis=1, inplace=True)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Model training\n\n* 2 models will be trained (NUM_MODELS=2)\n* Make predictions at the end of each epoch\n* Weighted averaging epoch predictions\n* Weights = 2 ** epoch"},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncheckpoint_predictions = []\nweights = []\nNUM_MODELS = 2\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1], loss_weight)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            [\n                x_train \n            ,x_tr_real\n            ],\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1,\n            callbacks = [\n                LearningRateScheduler(lambda _: 1e-3*(0.55**global_epoch)) # Decayed learning rate 0.55 best\n            ]\n        )\n        checkpoint_predictions.append(model.predict([x_test\n                                                        , x_te_real\n                                                        ], batch_size=2048)[0].flatten())\n        gc.collect()\n        weights.append(2 ** global_epoch)\n    del model\n    cuda.select_device(0)\n    cuda.close()\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\nlstm_submission = pd.DataFrame.from_dict({\n    'id': test.id,\n    'prediction': predictions\n})\n#lstm_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n'id': test['id'],\n'prediction': lstm_submission['prediction'].rank(pct=True)*0.3 + bert_submission['prediction'].rank(pct=True)*0.7})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}